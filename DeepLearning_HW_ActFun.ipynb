{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_HW_ActFun.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/kovandras/ActFun/blob/master/DeepLearning_HW_ActFun.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "QtXge4dgxISZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Multiple gated activation function \n",
        "###Deep learning Homework ActFun\n",
        "\n",
        "This homework's topic has been discussed with dr. Gyires-Tóth Bálint.\n",
        "\n",
        "The topic of this homework is to evaluate the effectiveness of a new type of activation function which is a superposition of previously established activation functions (tanh, relu, sigmoid...) with learnable weights. From now on referred to as WSGAF (Weighted Superposition Gated Function)\n",
        "\n",
        "![Weighted superposition gated activation function](https://i.imgur.com/QF58qzV.png)\n",
        "\n",
        "We expect this new method to outperform previous heuristics to determine the best activation to use in a certain layer, because this method gets rid of this hyperparameter for every layer and makes the network learn it for itself.\n",
        "\n",
        "We will evaluate two ways to do this. The first method (WSGAF-1) is to use separate weights for each neuron. (This method should provide more flexibility for the network, but is also expected to substantially increase learning times, because of the extra weights.)\n",
        "The second method (WSGAF-2) uses the same weights per layer. (This format is similar to existing structures)\n",
        "\n",
        "## Previous works\n",
        "\n",
        "We have not found any previous implementation of this idea, however success has been achieved with a similar self-gated activation function [arXiv:1710.05941 [cs.NE]](https://arxiv.org/abs/1710.05941). \n",
        "\n",
        "##Evaluation\n",
        "\n",
        "To evaluate this method, we are going to use a simple six-layer convolutional network (784-50-100-500-1000-10-10) on the MNIST database and Inception ResNet-v2 with CIFAR-10 dataset. These networks cover a wide range of layer types including Convolution, AvgPool, MaxPool, Concat, Dropout, FullyConnected, SoftMax. First, we are going to determine baseline performance of these networks by training them with nominal hyperparameter settings, then we are going to replace all of the activation function in these networks with WSGAF-1/2.\n",
        "\n",
        "We are going to compare the resulting: \n",
        "\n",
        "*   Training time\n",
        "*   Performance after a fix number of epochs\n",
        "\n",
        "for each version of both networks.\n",
        "Even if our results are on par with the baseline scores, our method will be considered preferable to other activation functions, because this method decreases the hyperparameter search space. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AXKCRvXP7O4J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.slim.nets import resnet_v2\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zyDi9-FBHYHl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MNIST dataset preprocessing\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "\n",
        "x_train_mnist = x_train_mnist.reshape(-1, 784)\n",
        "x_test_mnist = x_test_mnist.reshape(-1, 784)\n",
        "x_train_mnist = x_train_mnist.astype('float32')\n",
        "x_test_mnist = x_test_mnist.astype('float32')\n",
        "\n",
        "y_train_mnist = y_train_mnist.reshape(-1, 1)\n",
        "y_test_mnist = y_test_mnist.reshape(-1, 1)\n",
        "\n",
        "#Convert output to one-hot coding\n",
        "y_train_mnist = np_utils.to_categorical(y_train_mnist, 10)\n",
        "y_test_mnist = np_utils.to_categorical(y_test_mnist, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7cWJxBH7P_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CIFAR-10\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train_cifar10, y_train_cifar10), (x_test_cifar10, y_test_cifar10) = cifar10.load_data()\n",
        "\n",
        "#Convert output to one-hot coding\n",
        "y_train_cifar10 = np_utils.to_categorical(y_train_cifar10, 10)\n",
        "y_test_cifar10 = np_utils.to_categorical(y_test_cifar10, 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBz93qqwK0nF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}